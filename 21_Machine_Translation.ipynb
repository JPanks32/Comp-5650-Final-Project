{"cells":[{"cell_type":"markdown","metadata":{"id":"iNOjTAEGFIvW"},"source":["# TensorFlow Tutorial #21\n","# Machine Translation\n","\n","by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n","/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2864,"status":"ok","timestamp":1647009426912,"user":{"displayName":"Bayu Sukmanto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13560974109474990649"},"user_tz":360},"id":"OYdIwKR1Yt2m","outputId":"22247e8d-7270-42f9-ebf4-beeddf3a6ae5"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","# enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = 'cs231n/assignments/ai'\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","# now that we've mounted your Drive, this ensures that\n","# the Python interpreter of the Colab VM can load\n","# python files within it.\n","import sys\n","sys.path.append('/content.drive/My Drive/{}'.format(FOLDERNAME))\n","\n","# this downloads the CIFAR-10 dataset to your Drive\n","# if it doesn't already exist\n","%cd drive/My\\ Drive/$FOLDERNAME/datasets/\n","!python setup.py build_ext --inplace\n","%cd ../\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"rHpjCpLAFIvd"},"source":["## Introduction\n","\n","Tutorial #20 showed how to use a Recurrent Neural Network (RNN) to do so-called sentiment analysis on texts of movie reviews. This tutorial will extend that idea to do Machine Translation of human languages by combining two RNN's.\n","\n","You should be familiar with TensorFlow, Keras and the basics of Natural Language Processing, see Tutorials #01, #03-C and #20."]},{"cell_type":"markdown","metadata":{"id":"lmzRZS7EFIve"},"source":["## Flowchart\n","\n","The following flowchart shows roughly how the neural network is constructed. It is split into two parts: An encoder which maps the source-text to a \"thought vector\" that summarizes the text's contents, which is then input to the second part of the neural network that decodes the \"thought vector\" to the destination-text.\n","\n","The neural network cannot work directly on text so first we need to convert each word to an integer-token using a tokenizer. But the neural network cannot work on integers either, so we use a so-called Embedding Layer to convert each integer-token to a vector of floating-point values. The embedding is trained alongside the rest of the neural network to map words with similar semantic meaning to similar vectors of floating-point values.\n","\n","For example, consider the Danish text \"der var engang\" which is the beginning of any fairytale and literally means \"there was once\" but is commonly translated into English as \"once upon a time\". We first convert the entire data-set to integer-tokens so the text \"der var engang\" becomes [12, 54, 1097]. Each of these integer-tokens is then mapped to an embedding-vector with e.g. 128 elements, so the integer-token 12 could for example become [0.12, -0.56, ..., 1.19] and the integer-token 54 could for example become [0.39, 0.09, ..., -0.12]. These embedding-vectors can then be input to the Recurrent Neural Network, which has 3 GRU-layers. See Tutorial #20 for a more detailed explanation.\n","\n","The last GRU-layer outputs a single vector - the \"thought vector\" that summarizes the contents of the source-text - which is then used as the initial state of the GRU-units in the decoder-part.\n","\n","The destination-text \"once upon a time\" is padded with special markers \"ssss\" and \"eeee\" to indicate its beginning and end, so the sequence of integer-tokens becomes [2, 337, 640, 9, 79, 3]. During training, the decoder will be given this entire sequence as input and  the desired output sequence is [337, 640, 9, 79, 3] which is the same sequence but time-shifted one step. We are trying to teach the decoder to map the \"thought vector\" and the start-token \"ssss\" (integer 2) to the next word \"once\" (integer 337), and then map the word \"once\" to the word \"upon\" (integer 640), and so forth.\n","\n","This flow-chart depicts the main idea but does not show all the necessary details e.g. regarding the loss function which is also somewhat complicated."]},{"cell_type":"markdown","metadata":{"id":"bhL0JmIVFIvg"},"source":["![Flowchart](images/21_machine_translation_flowchart.png)"]},{"cell_type":"markdown","metadata":{"id":"mOV-2TgfFIvh"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#To create local environment, go to the terminal and make sure the directory is on the folder containing this file. \n","# Type \"python3 -m venv env\" into the terminal to create the local environment. \n","%pip install matplotlib\n","%pip install tensorflow\n","%pip install numpy\n","%pip install datasets\n","%pip install zstandard\n","%pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"esbpHfolFIvi"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","import math\n","import os"]},{"cell_type":"markdown","metadata":{"id":"ZBFwzVLxFIvl"},"source":["We need to import several things from Keras."]},{"cell_type":"code","execution_count":35,"metadata":{"id":"pH_9GA9zFIvm"},"outputs":[],"source":["# from tf.keras.models import Model  # This does not work!\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, GRU, Embedding, Dropout\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","metadata":{"id":"jfGQWykXFIvn"},"source":["This was developed using Python 3.6 (Anaconda) and package versions:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":248,"status":"ok","timestamp":1647009446898,"user":{"displayName":"Bayu Sukmanto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13560974109474990649"},"user_tz":360},"id":"cKCyiCFOFIvo","outputId":"33d59fce-1e8f-4613-9e71-c7c35f8ed311","scrolled":false},"outputs":[{"data":{"text/plain":["'2.8.0'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1647009449224,"user":{"displayName":"Bayu Sukmanto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13560974109474990649"},"user_tz":360},"id":"yMP5QN-gFIvp","outputId":"af3d4f0a-1db5-4764-af4e-2c050f025860","scrolled":true},"outputs":[{"data":{"text/plain":["'2.8.0'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["tf.keras.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3947,"status":"ok","timestamp":1647010153469,"user":{"displayName":"Bayu Sukmanto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13560974109474990649"},"user_tz":360},"id":"KbkS0sqJXomi","outputId":"afc10d24-1aa5-4f0c-f03e-3b4100eb4235"},"outputs":[],"source":["#!pip install zstandard"]},{"cell_type":"markdown","metadata":{"id":"XrbNQAU9FIvq"},"source":["## Load Data\n","\n","#We will use the huggingface dataset Wikidepia/IndoParaCrawl\n","\n","#https://huggingface.co/datasets/Wikidepia/IndoParaCrawl"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["27e71996fd8b45ed8e17ed013728728d","92684d53ff5d48f886db72d6e9f66081","0aa9a7fd8caf4ec9bfca3e068a9bd00d","e2467eb99aae4404a4bf9377c4906451","9f508c40f4b74a50bf4205fda6df06c3","db4230021f97435396ac427400dbabfc","a4eba7b6a6a34a2aa93e7201e47564a5","cab347bf122d468d89c9b387b280c874","1b43c4170f2e40a99e6a8cba1ce0a2d5","5ba67a4e8ae94e3b887d6a82c976f8e2","cee6be8bc3ff4d6eb70bf0971d053661","6fe6122d9a6e4b2f9988525007b95d70","d4890aee06034969a85fa1362e331fee","a57acaa9a4124464acd4d5581b6a976f","4288cd7a677f4a79bf6af19c52aff64b","f41ff3961e7a491a832fbcb0711f8f41","52dc7ef1f16e471998ea714c5c11297c","40844d7d58654cbab7373e2ae7589229","3ad9a88eff574ef4b09eb74472810410","0e70b29c14f04d90a01581b0d8126364","471ac32f51254cb2afed0a535fb29f16","aac0261e8b4b44b8b177f2a9f8a295ef"]},"executionInfo":{"elapsed":623501,"status":"error","timestamp":1647010782297,"user":{"displayName":"Bayu Sukmanto","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13560974109474990649"},"user_tz":360},"id":"MtRjmE1wFIvr","outputId":"b8823aad-2cb6-4cec-9633-29807e556b84"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration Wikidepia--IndoParaCrawl-46e41816710df16a\n","Reusing dataset json (C:\\Users\\panke\\.cache\\huggingface\\datasets\\json\\Wikidepia--IndoParaCrawl-46e41816710df16a\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18c1c27a4bf546eeabfb09e1beaee343","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#!pip install datasets\n","#!pip install zstandard\n","from datasets import load_dataset\n","\n","#from datasets.filesystems import S3FileSystem\n","#dataset = load_dataset(\"Wikidepia/IndoParaCrawl\", data_files = 'IndoParaCrawl-1*.jsonl.zst')\n","#dataset_small = load_dataset(\"Wikidepia/IndoParaCrawl\", data_files = 'IndoParaCrawl-5*.jsonl.zst')\n","dataset_tiny = load_dataset(\"Wikidepia/IndoParaCrawl\", data_files = 'IndoParaCrawl-11.jsonl.zst')\n","#dataset.save_to_disk('/dataset')\n","\n","#Wikidepia/IndoParaCrawl\n","#import europarl"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#books = load_dataset(\"opus_books\", \"en-fr\")\n","\n","#books = books[\"train\"].train_test_split(test_size=0.2)\n","#_dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n","#dataset_small = _dataset[\"test\"]\n","#_dataset_small = dataset_small.train_test_split(test_size=0.2)\n","#dataset_tiny = _dataset_small[\"test\"]\n","#_dataset_tiny = dataset_tiny.train_test_split(test_size=0.2)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["_dataset_tiny = dataset_tiny['train'].train_test_split(test_size=0.1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['en', 'id'],\n","        num_rows: 7999898\n","    })\n","    test: Dataset({\n","        features: ['en', 'id'],\n","        num_rows: 1999975\n","    })\n","})\n"]}],"source":["print(_dataset_tiny)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["_dataset_sample = _dataset_tiny['test']"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["dataset_micro = _dataset_tiny[\"test\"].train_test_split(test_size=0.001)['test']"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n","_dataset_micro = dataset_micro.train_test_split(test_size=0.1)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['en', 'id'],\n","    num_rows: 1999975\n","})\n","DatasetDict({\n","    train: Dataset({\n","        features: ['en', 'id'],\n","        num_rows: 1800\n","    })\n","    test: Dataset({\n","        features: ['en', 'id'],\n","        num_rows: 200\n","    })\n","})\n"]}],"source":["print(_dataset_sample)\n","print(_dataset_micro)"]},{"cell_type":"markdown","metadata":{"id":"zVUaHUFBFIvs"},"source":["In order for the decoder to know when to begin and end a sentence, we need to mark the start and end of each sentence with words that most likely don't occur in the data-set.\n","\n","https://keras.io/api/models/model_training_apis/#fit-method\n","\n","https://huggingface.co/docs/transformers/tasks/translation"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["_dataset_run = _dataset_tiny['test']"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"7Exm_aQwFIvt"},"outputs":[],"source":["mark_start = 'ssss '\n","mark_end = ' eeee'\n","s_lang = 'id'\n","d_lang = 'en'\n","dataset_rnn = _dataset_run"]},{"cell_type":"markdown","metadata":{"id":"qTpU4Rocy4Rr"},"source":["##Begin Train with Transformers"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Eti6UqPRmU_1"},"outputs":[],"source":["from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n","\n","_model_link = \"Helsinki-NLP/opus-mt-id-en\"\n","tokenizer = AutoTokenizer.from_pretrained(_model_link, return_tensors=\"tf\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(_model_link)\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import transformers\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForSeq2Seq\n","max_input_length = 128\n","max_target_length = 128\n","\n","\n","prefix = 'translate en to id: '\n","def preprocess(data):\n","  inputs = [example for example in data[s_lang]]\n","  targets = [example for example in data[d_lang]]\n","  model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n","  with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(targets, max_length=128, truncation=True)\n","  model_inputs['labels'] = labels['input_ids']\n","  return model_inputs\n","\n","def preprocess_commas(data):\n","  inputs = [example.replace(\",\", \" ,\").replace(\";\", \" ;\").replace(':', \" :\").replace('.', ' .').replace('?', \" ?\").replace(\"!\", \" !\").replace(\"'\", \" '\").replace(\"(\", \" (\").replace(\")\", \" )\") for example in data[s_lang]]\n","  targets = [example.replace(\",\", \" ,\").replace(\";\", \" ;\").replace(':', \" :\").replace('.', ' .').replace('?', \" ?\").replace(\"!\", \" !\").replace(\"'\", \" '\").replace(\"(\", \" (\").replace(\")\", \" )\") for example in data[d_lang]]\n","  model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n","  with tokenizer.as_target_tokenizer():\n","    labels = tokenizer(targets, max_length=128, truncation=True)\n","  model_inputs['labels'] = labels['input_ids']\n","  return model_inputs\n","\n","\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"250bd4cba7eb4d0f973c197456303327","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 34'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000033?line=0'>1</a>\u001b[0m tokenized_data \u001b[39m=\u001b[39m _dataset_run\u001b[39m.\u001b[39;49mmap(preprocess, batched\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, keep_in_memory\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:2109\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2105'>2106</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mbool\u001b[39m(logging\u001b[39m.\u001b[39mget_verbosity() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m utils\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2107'>2108</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2108'>2109</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2109'>2110</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2110'>2111</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2111'>2112</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2112'>2113</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2113'>2114</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2114'>2115</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2115'>2116</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2116'>2117</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2117'>2118</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2118'>2119</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2119'>2120</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2120'>2121</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2121'>2122</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2122'>2123</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2123'>2124</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2124'>2125</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2125'>2126</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2126'>2127</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2127'>2128</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2128'>2129</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2130'>2131</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:518\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=515'>516</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=516'>517</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=517'>518</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=518'>519</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=519'>520</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=520'>521</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:485\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=477'>478</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=478'>479</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=479'>480</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=480'>481</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=481'>482</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=482'>483</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=483'>484</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=484'>485</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=485'>486</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=486'>487</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\fingerprint.py:413\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/fingerprint.py?line=406'>407</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/fingerprint.py?line=407'>408</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/fingerprint.py?line=408'>409</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/fingerprint.py?line=410'>411</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/fingerprint.py?line=412'>413</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/fingerprint.py?line=414'>415</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/fingerprint.py?line=416'>417</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:2488\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2483'>2484</a>\u001b[0m indices \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2484'>2485</a>\u001b[0m     \u001b[39mrange\u001b[39m(\u001b[39m*\u001b[39m(\u001b[39mslice\u001b[39m(i, i \u001b[39m+\u001b[39m batch_size)\u001b[39m.\u001b[39mindices(input_dataset\u001b[39m.\u001b[39mnum_rows)))\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2485'>2486</a>\u001b[0m )  \u001b[39m# Something simpler?\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2486'>2487</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2487'>2488</a>\u001b[0m     batch \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2488'>2489</a>\u001b[0m         batch,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2489'>2490</a>\u001b[0m         indices,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2490'>2491</a>\u001b[0m         check_same_num_examples\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(input_dataset\u001b[39m.\u001b[39;49mlist_indexes()) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2491'>2492</a>\u001b[0m         offset\u001b[39m=\u001b[39;49moffset,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2492'>2493</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2493'>2494</a>\u001b[0m \u001b[39mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2494'>2495</a>\u001b[0m     \u001b[39mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2495'>2496</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2496'>2497</a>\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:2374\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2371'>2372</a>\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2372'>2373</a>\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2373'>2374</a>\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2374'>2375</a>\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2375'>2376</a>\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2376'>2377</a>\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:2069\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[1;34m(item, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2064'>2065</a>\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2065'>2066</a>\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2066'>2067</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2067'>2068</a>\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2068'>2069</a>\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2069'>2070</a>\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=2070'>2071</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 31'\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000030?line=9'>10</a>\u001b[0m inputs \u001b[39m=\u001b[39m [example \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m data[s_lang]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000030?line=10'>11</a>\u001b[0m targets \u001b[39m=\u001b[39m [example \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m data[d_lang]]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000030?line=11'>12</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m tokenizer(inputs, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000030?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m tokenizer\u001b[39m.\u001b[39mas_target_tokenizer():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000030?line=13'>14</a>\u001b[0m   labels \u001b[39m=\u001b[39m tokenizer(targets, max_length\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"]}],"source":["tokenized_data = _dataset_run.map(preprocess, batched=True, keep_in_memory=True)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b687b742c6254ce7be8a3fe9e3056b99","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"322fd9fdc9c54a28a325a4210340d62f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_data_2 = _dataset_run.map(preprocess, batched=True, keep_in_memory=True, remove_columns=_dataset_run[\"train\"].column_names)"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[  131, 22873,    14, 35752,     3,    18,  5667,    39, 39204,    18,\n","         17337,  8881,   223,     3,    18,     3,  1095,   109,     3,  2539,\n","         15659,     0, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795],\n","        [ 9248,  1911,   981, 42758,   127,   836,   381, 32498,    19,  5462,\n","          2539, 31403, 33110,    18,    19,  7895,     2,     0, 54795, 54795,\n","         54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795],\n","        [ 4974,  4083,    21,  5563,  2352,  4055,  5804,  2850, 22169,  2853,\n","          1716,     9,   520,  1009,   127,  4589, 19506,     9,  1679, 17447,\n","         28252, 39730, 46373,     2,     0, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795],\n","        [19556,   142,  4196,    39,  6003, 21234, 12152,     3, 51662,     3,\n","          8117,    28, 11877,   204,    18,  4906,   246,     0, 54795, 54795,\n","         54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795],\n","        [14464,    49,  8892,    18, 11818, 10121,    11,   166,    18,  7017,\n","         12297,    23,   168,   577, 23041,   133,     3,   176,    67,   217,\n","          4246,   441,   758,    11,   979,    18,   758,    11,   979,    41,\n","           134,     2,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  197, 37246,    14,     9,  5700,   143,     9,     3,    17, 13949,\n","            27,  1291,    12, 31279,    17,   644,  1083, 13712,     6,  1194,\n","             3,    17,     3,    16,   642,     3,     6, 24312,   299,     0,\n","          -100,  -100,  -100,  -100,  -100],\n","        [   88, 11933,  9958,    16, 28570,  5650,    47,   174, 19658,    27,\n","           190,  2375, 33110,  2876,  3160,    17,    27,     6,  7409,     9,\n","             2,     0,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100],\n","        [ 1580,     2,   756,     2,    88,  4083,    21,  2016,  2352,  4055,\n","          5804,  2850,  2853,  1716,     9, 22932,   129,    88, 28307,   555,\n","          8334,    30,   723,  6446,  5022, 14139, 49105,     2,     0,  -100,\n","          -100,  -100,  -100,  -100,  -100],\n","        [22284,  9270,    40,  4379,     9,  7458,  9696,     3,  1904, 20999,\n","             3,    28, 39813,    17,  5900,   246,  2357,     0,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100],\n","        [   88, 10671,    16, 25379,   184,    17,  3034,  2489,  6255,  4626,\n","         23619,    17,  7072, 19790,  1252,    46,   271,  1106, 12844, 22402,\n","             3,  1089,   108,    30,   355,  3653,   953,   373,   524,    17,\n","           373,   394,   524,     2,     0]]), 'decoder_input_ids': tensor([[54795,   197, 37246,    14,     9,  5700,   143,     9,     3,    17,\n","         13949,    27,  1291,    12, 31279,    17,   644,  1083, 13712,     6,\n","          1194,     3,    17,     3,    16,   642,     3,     6, 24312,   299,\n","             0, 54795, 54795, 54795, 54795],\n","        [54795,    88, 11933,  9958,    16, 28570,  5650,    47,   174, 19658,\n","            27,   190,  2375, 33110,  2876,  3160,    17,    27,     6,  7409,\n","             9,     2,     0, 54795, 54795, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795, 54795, 54795],\n","        [54795,  1580,     2,   756,     2,    88,  4083,    21,  2016,  2352,\n","          4055,  5804,  2850,  2853,  1716,     9, 22932,   129,    88, 28307,\n","           555,  8334,    30,   723,  6446,  5022, 14139, 49105,     2,     0,\n","         54795, 54795, 54795, 54795, 54795],\n","        [54795, 22284,  9270,    40,  4379,     9,  7458,  9696,     3,  1904,\n","         20999,     3,    28, 39813,    17,  5900,   246,  2357,     0, 54795,\n","         54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795, 54795, 54795],\n","        [54795,    88, 10671,    16, 25379,   184,    17,  3034,  2489,  6255,\n","          4626, 23619,    17,  7072, 19790,  1252,    46,   271,  1106, 12844,\n","         22402,     3,  1089,   108,    30,   355,  3653,   953,   373,   524,\n","            17,   373,   394,   524,     2]])}\n"]}],"source":["print(data_collator([tokenized_data_2[\"train\"][i] for i in range(5)]))"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input_ids shape: torch.Size([5, 33])\n","attention_mask shape: torch.Size([5, 33])\n","labels shape: torch.Size([5, 35])\n","decoder_input_ids shape: torch.Size([5, 35])\n"]}],"source":["out = data_collator([tokenized_data_2[\"train\"][i] for i in range(5)])\n","for key in out:\n","    print(f\"{key} shape: {out[key].shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#tokenized_data_pico_2 = _dataset_pico.map(preprocess, batched=True, remove_columns=_dataset_pico[\"train\"].column_names,)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['en', 'id'],\n","        num_rows: 1800\n","    })\n","    test: Dataset({\n","        features: ['en', 'id'],\n","        num_rows: 200\n","    })\n","})\n"]}],"source":["print(_dataset_micro)\n","#print(_dataset_pico)"]},{"cell_type":"markdown","metadata":{"id":"tseniiKRycx-"},"source":["## Option 1: Fine tune with Trainer"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["dict_keys(['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'])"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["batch = data_collator([tokenized_data_2[\"train\"][i] for i in range(1, 3)])\n","batch.keys()"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["batch[\"attention_mask\"]"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[54795,  1015,    51,  1309,    27,  6008,    84,  1605,  3769,  4674,\n","             7,  2376,     9,  3324,  4423,   118,  8351,  4538,     0, 54795,\n","         54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795,\n","         54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795, 54795],\n","        [54795,  9493,   184,  7042,   338,  8720,     9,    17,  2012,     9,\n","             3, 14493, 30182, 33544,   858,  3470,  6981,  1565,    12, 15824,\n","          2141,  1560,  1648,  1006,    27,   140,   373,    17,   561,  5498,\n","           125,   157, 21813,    40,   500,    16,     6, 19119,  3072,    12,\n","             6,  2384,  3978,     3,  3483,    97,  4567, 18553,     2]])"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["batch[\"decoder_input_ids\"]"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1015, 51, 1309, 27, 6008, 84, 1605, 3769, 4674, 7, 2376, 9, 3324, 4423, 118, 8351, 4538, 0]\n","[9493, 184, 7042, 338, 8720, 9, 17, 2012, 9, 3, 14493, 30182, 33544, 858, 3470, 6981, 1565, 12, 15824, 2141, 1560, 1648, 1006, 27, 140, 373, 17, 561, 5498, 125, 157, 21813, 40, 500, 16, 6, 19119, 3072, 12, 6, 2384, 3978, 3, 3483, 97, 4567, 18553, 2, 0]\n"]}],"source":["for i in range(1, 3):\n","    print(tokenized_data_2[\"train\"][i][\"labels\"])"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"sacrebleu\")\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    # In case the model returns more than the prediction logits\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # Replace -100s in the labels as we can't decode them\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Some simple post-processing\n","    decoded_preds = [pred.strip() for pred in decoded_preds]\n","    decoded_labels = [[label.strip()] for label in decoded_labels]\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n","    return {\"bleu\": result[\"score\"]}"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"WpzSDt1OsTmO"},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","#model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n","#data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","max_input_length = 128\n","max_target_length = 128\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"no\",\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=3,\n","    fp16=False, #change fp16 to false if asking for Cuda\n",")\n","#print(\"test2\")\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_data_2[\"train\"],\n","    eval_dataset=tokenized_data_2[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 200\n","  Batch size = 4\n"]},{"ename":"TypeError","evalue":"int() argument must be a string, a bytes-like object or a number, not 'list'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 45'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000044?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate(max_length\u001b[39m=\u001b[39;49mmax_target_length)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\trainer_seq2seq.py:70\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, max_length, num_beams)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer_seq2seq.py?line=67'>68</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_length \u001b[39m=\u001b[39m max_length \u001b[39mif\u001b[39;00m max_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_max_length\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer_seq2seq.py?line=68'>69</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_beams \u001b[39m=\u001b[39m num_beams \u001b[39mif\u001b[39;00m num_beams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_num_beams\n\u001b[1;32m---> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer_seq2seq.py?line=69'>70</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mevaluate(eval_dataset, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2280'>2281</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2282'>2283</a>\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2283'>2284</a>\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2284'>2285</a>\u001b[0m     eval_dataloader,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2285'>2286</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2286'>2287</a>\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2287'>2288</a>\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2288'>2289</a>\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2289'>2290</a>\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2290'>2291</a>\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2291'>2292</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2293'>2294</a>\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2294'>2295</a>\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2295'>2296</a>\u001b[0m     speed_metrics(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2296'>2297</a>\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2300'>2301</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2301'>2302</a>\u001b[0m )\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\trainer.py:2535\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2532'>2533</a>\u001b[0m \u001b[39m# Metrics!\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2533'>2534</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m all_preds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m all_labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2534'>2535</a>\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2535'>2536</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2536'>2537</a>\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 42'\u001b[0m in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_preds)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(preds, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=9'>10</a>\u001b[0m     preds \u001b[39m=\u001b[39m preds[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=11'>12</a>\u001b[0m decoded_preds \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_decode(preds, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=13'>14</a>\u001b[0m \u001b[39m# Replace -100s in the labels as we can't decode them\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=14'>15</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(labels \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m, labels, tokenizer\u001b[39m.\u001b[39mpad_token_id)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:241\u001b[0m, in \u001b[0;36mMarianTokenizer.batch_decode\u001b[1;34m(self, sequences, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=220'>221</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\u001b[39mself\u001b[39m, sequences, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=221'>222</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=222'>223</a>\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=223'>224</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=238'>239</a>\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=239'>240</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=240'>241</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mbatch_decode(sequences, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3265\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3241'>3242</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3242'>3243</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3243'>3244</a>\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3246'>3247</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3247'>3248</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3248'>3249</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3249'>3250</a>\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3250'>3251</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3262'>3263</a>\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3263'>3264</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3264'>3265</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3265'>3266</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3266'>3267</a>\u001b[0m             seq,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3267'>3268</a>\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3268'>3269</a>\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3269'>3270</a>\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3270'>3271</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3271'>3272</a>\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3272'>3273</a>\u001b[0m     ]\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3266\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3241'>3242</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3242'>3243</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3243'>3244</a>\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3246'>3247</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3247'>3248</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3248'>3249</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3249'>3250</a>\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3250'>3251</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3262'>3263</a>\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3263'>3264</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3264'>3265</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3265'>3266</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3266'>3267</a>\u001b[0m             seq,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3267'>3268</a>\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3268'>3269</a>\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3269'>3270</a>\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3270'>3271</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3271'>3272</a>\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3272'>3273</a>\u001b[0m     ]\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:266\u001b[0m, in \u001b[0;36mMarianTokenizer.decode\u001b[1;34m(self, token_ids, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=242'>243</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, token_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=243'>244</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=244'>245</a>\u001b[0m \u001b[39m    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=245'>246</a>\u001b[0m \u001b[39m    tokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=263'>264</a>\u001b[0m \u001b[39m        `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=264'>265</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=265'>266</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdecode(token_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3304\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3300'>3301</a>\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3301'>3302</a>\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3303'>3304</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3304'>3305</a>\u001b[0m     token_ids\u001b[39m=\u001b[39mtoken_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3305'>3306</a>\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3306'>3307</a>\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3307'>3308</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3308'>3309</a>\u001b[0m )\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils.py:928\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=917'>918</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=918'>919</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=919'>920</a>\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=923'>924</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=924'>925</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=925'>926</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=927'>928</a>\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=929'>930</a>\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=930'>931</a>\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=931'>932</a>\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=932'>933</a>\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils.py:903\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=900'>901</a>\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=901'>902</a>\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=902'>903</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=903'>904</a>\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=904'>905</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n","\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"]}],"source":["trainer.evaluate(max_length=max_target_length)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save_pretrained(\"./Dataset\")"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 1800\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1350\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ea0be1c865d4052a7bf9460a6f3c75d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1350 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./results\\checkpoint-450\n","Configuration saved in ./results\\checkpoint-450\\config.json\n","Model weights saved in ./results\\checkpoint-450\\pytorch_model.bin\n","tokenizer config file saved in ./results\\checkpoint-450\\tokenizer_config.json\n","Special tokens file saved in ./results\\checkpoint-450\\special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.4588, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.11}\n"]},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./results\\checkpoint-900\n","Configuration saved in ./results\\checkpoint-900\\config.json\n","Model weights saved in ./results\\checkpoint-900\\pytorch_model.bin\n","tokenizer config file saved in ./results\\checkpoint-900\\tokenizer_config.json\n","Special tokens file saved in ./results\\checkpoint-900\\special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["{'loss': 1.0547, 'learning_rate': 5.185185185185185e-06, 'epoch': 2.22}\n"]},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./results\\checkpoint-1350\n","Configuration saved in ./results\\checkpoint-1350\\config.json\n","Model weights saved in ./results\\checkpoint-1350\\pytorch_model.bin\n","tokenizer config file saved in ./results\\checkpoint-1350\\tokenizer_config.json\n","Special tokens file saved in ./results\\checkpoint-1350\\special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["{'train_runtime': 2407.5201, 'train_samples_per_second': 2.243, 'train_steps_per_second': 0.561, 'train_loss': 1.170412168149595, 'epoch': 3.0}\n"]},{"data":{"text/plain":["TrainOutput(global_step=1350, training_loss=1.170412168149595, metrics={'train_runtime': 2407.5201, 'train_samples_per_second': 2.243, 'train_steps_per_second': 0.561, 'train_loss': 1.170412168149595, 'epoch': 3.0})"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"VeXv2h7jyVL4"},"source":["#Option 2: Fine tune with TensorFlow\n"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["***** Running Evaluation *****\n","  Num examples = 200\n","  Batch size = 4\n"]},{"ename":"TypeError","evalue":"int() argument must be a string, a bytes-like object or a number, not 'list'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 52'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000177?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mevaluate()\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\trainer_seq2seq.py:70\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, max_length, num_beams)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer_seq2seq.py?line=67'>68</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_length \u001b[39m=\u001b[39m max_length \u001b[39mif\u001b[39;00m max_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_max_length\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer_seq2seq.py?line=68'>69</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_beams \u001b[39m=\u001b[39m num_beams \u001b[39mif\u001b[39;00m num_beams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_num_beams\n\u001b[1;32m---> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer_seq2seq.py?line=69'>70</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mevaluate(eval_dataset, ignore_keys\u001b[39m=\u001b[39;49mignore_keys, metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\trainer.py:2284\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2280'>2281</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2282'>2283</a>\u001b[0m eval_loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction_loop \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39muse_legacy_prediction_loop \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_loop\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2283'>2284</a>\u001b[0m output \u001b[39m=\u001b[39m eval_loop(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2284'>2285</a>\u001b[0m     eval_dataloader,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2285'>2286</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEvaluation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2286'>2287</a>\u001b[0m     \u001b[39m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2287'>2288</a>\u001b[0m     \u001b[39m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2288'>2289</a>\u001b[0m     prediction_loss_only\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2289'>2290</a>\u001b[0m     ignore_keys\u001b[39m=\u001b[39;49mignore_keys,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2290'>2291</a>\u001b[0m     metric_key_prefix\u001b[39m=\u001b[39;49mmetric_key_prefix,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2291'>2292</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2293'>2294</a>\u001b[0m total_batch_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39meval_batch_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mworld_size\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2294'>2295</a>\u001b[0m output\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2295'>2296</a>\u001b[0m     speed_metrics(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2296'>2297</a>\u001b[0m         metric_key_prefix,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2300'>2301</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2301'>2302</a>\u001b[0m )\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\trainer.py:2535\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2532'>2533</a>\u001b[0m \u001b[39m# Metrics!\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2533'>2534</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m all_preds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m all_labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2534'>2535</a>\u001b[0m     metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_metrics(EvalPrediction(predictions\u001b[39m=\u001b[39;49mall_preds, label_ids\u001b[39m=\u001b[39;49mall_labels))\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2535'>2536</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/trainer.py?line=2536'>2537</a>\u001b[0m     metrics \u001b[39m=\u001b[39m {}\n","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 45'\u001b[0m in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_preds)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(preds, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=9'>10</a>\u001b[0m     preds \u001b[39m=\u001b[39m preds[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=11'>12</a>\u001b[0m decoded_preds \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mbatch_decode(preds, skip_special_tokens\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=13'>14</a>\u001b[0m \u001b[39m# Replace -100s in the labels as we can't decode them\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000041?line=14'>15</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(labels \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m, labels, tokenizer\u001b[39m.\u001b[39mpad_token_id)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:241\u001b[0m, in \u001b[0;36mMarianTokenizer.batch_decode\u001b[1;34m(self, sequences, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=220'>221</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\u001b[39mself\u001b[39m, sequences, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=221'>222</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=222'>223</a>\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=223'>224</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=238'>239</a>\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=239'>240</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=240'>241</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mbatch_decode(sequences, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3265\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3241'>3242</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3242'>3243</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3243'>3244</a>\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3246'>3247</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3247'>3248</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3248'>3249</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3249'>3250</a>\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3250'>3251</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3262'>3263</a>\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3263'>3264</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3264'>3265</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3265'>3266</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3266'>3267</a>\u001b[0m             seq,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3267'>3268</a>\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3268'>3269</a>\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3269'>3270</a>\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3270'>3271</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3271'>3272</a>\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3272'>3273</a>\u001b[0m     ]\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3266\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3241'>3242</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_decode\u001b[39m(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3242'>3243</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3243'>3244</a>\u001b[0m     sequences: Union[List[\u001b[39mint\u001b[39m], List[List[\u001b[39mint\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mnp.ndarray\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtorch.Tensor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtf.Tensor\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3246'>3247</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3247'>3248</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3248'>3249</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3249'>3250</a>\u001b[0m \u001b[39m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3250'>3251</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3262'>3263</a>\u001b[0m \u001b[39m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3263'>3264</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3264'>3265</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3265'>3266</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3266'>3267</a>\u001b[0m             seq,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3267'>3268</a>\u001b[0m             skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3268'>3269</a>\u001b[0m             clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3269'>3270</a>\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3270'>3271</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3271'>3272</a>\u001b[0m         \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3272'>3273</a>\u001b[0m     ]\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:266\u001b[0m, in \u001b[0;36mMarianTokenizer.decode\u001b[1;34m(self, token_ids, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=242'>243</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, token_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=243'>244</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=244'>245</a>\u001b[0m \u001b[39m    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=245'>246</a>\u001b[0m \u001b[39m    tokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=263'>264</a>\u001b[0m \u001b[39m        `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=264'>265</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/models/marian/tokenization_marian.py?line=265'>266</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdecode(token_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3304\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3300'>3301</a>\u001b[0m \u001b[39m# Convert inputs to python lists\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3301'>3302</a>\u001b[0m token_ids \u001b[39m=\u001b[39m to_py_obj(token_ids)\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3303'>3304</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3304'>3305</a>\u001b[0m     token_ids\u001b[39m=\u001b[39mtoken_ids,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3305'>3306</a>\u001b[0m     skip_special_tokens\u001b[39m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3306'>3307</a>\u001b[0m     clean_up_tokenization_spaces\u001b[39m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3307'>3308</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils_base.py?line=3308'>3309</a>\u001b[0m )\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils.py:928\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=917'>918</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decode\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=918'>919</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=919'>920</a>\u001b[0m     token_ids: List[\u001b[39mint\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=923'>924</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=924'>925</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=925'>926</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decode_use_source_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39muse_source_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=927'>928</a>\u001b[0m     filtered_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_ids_to_tokens(token_ids, skip_special_tokens\u001b[39m=\u001b[39;49mskip_special_tokens)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=929'>930</a>\u001b[0m     \u001b[39m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=930'>931</a>\u001b[0m     \u001b[39m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=931'>932</a>\u001b[0m     \u001b[39m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=932'>933</a>\u001b[0m     sub_texts \u001b[39m=\u001b[39m []\n","File \u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\.env\\lib\\site-packages\\transformers\\tokenization_utils.py:903\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[1;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=900'>901</a>\u001b[0m tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=901'>902</a>\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m ids:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=902'>903</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(index)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=903'>904</a>\u001b[0m     \u001b[39mif\u001b[39;00m skip_special_tokens \u001b[39mand\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_special_ids:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/.env/lib/site-packages/transformers/tokenization_utils.py?line=904'>905</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n","\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"]}],"source":["trainer.evaluate()\n"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"3MRLENdFx8iF"},"outputs":[{"ename":"AttributeError","evalue":"'torch.Size' object has no attribute 'as_list'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 50'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=0'>1</a>\u001b[0m tf_train_set \u001b[39m=\u001b[39m tokenized_data_micro_2[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mto_tf_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=1'>2</a>\u001b[0m     columns\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlabels\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=2'>3</a>\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=3'>4</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=4'>5</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=5'>6</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=7'>8</a>\u001b[0m tf_test_set \u001b[39m=\u001b[39m tokenized_data_micro_2[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_tf_dataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=8'>9</a>\u001b[0m     columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=9'>10</a>\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=10'>11</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=11'>12</a>\u001b[0m     collate_fn\u001b[39m=\u001b[39mdata_collator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000048?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m create_optimizers, AdamWeightDecay\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:379\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin.to_tf_dataset\u001b[1;34m(self, columns, batch_size, shuffle, collate_fn, drop_remainder, collate_fn_args, label_cols, dummy_labels, prefetch)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=375'>376</a>\u001b[0m retained_columns \u001b[39m=\u001b[39m [key \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures\u001b[39m.\u001b[39mkeys() \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m cols_to_retain]\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=376'>377</a>\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_format(\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, columns\u001b[39m=\u001b[39mretained_columns)\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=378'>379</a>\u001b[0m columns_to_dtypes, output_signature \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_output_signature(\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=379'>380</a>\u001b[0m     dataset, collate_fn, collate_fn_args, batch_size\u001b[39m=\u001b[39;49mbatch_size \u001b[39mif\u001b[39;49;00m drop_remainder \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=380'>381</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=381'>382</a>\u001b[0m all_columns \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(columns_to_dtypes\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=382'>383</a>\u001b[0m all_dtypes \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(columns_to_dtypes\u001b[39m.\u001b[39mvalues())\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\datasets\\arrow_dataset.py:299\u001b[0m, in \u001b[0;36mTensorflowDatasetMixin._get_output_signature\u001b[1;34m(dataset, collate_fn, collate_fn_args, batch_size)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=294'>295</a>\u001b[0m             shape \u001b[39m=\u001b[39m [batch_size] \u001b[39m+\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mas_list()[\u001b[39m1\u001b[39m:]]\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=295'>296</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=296'>297</a>\u001b[0m         \u001b[39m# If this doesn't look like LM labels that got added by the collate_fn, let's not say anything\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=297'>298</a>\u001b[0m         \u001b[39m# about the dimensions we're unsure of\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=298'>299</a>\u001b[0m         shape \u001b[39m=\u001b[39m [batch_size] \u001b[39m+\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39;49mshape\u001b[39m.\u001b[39;49mas_list()[\u001b[39m1\u001b[39m:]]\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=299'>300</a>\u001b[0m     signatures[column] \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mTensorSpec(shape\u001b[39m=\u001b[39mshape, dtype\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/datasets/arrow_dataset.py?line=300'>301</a>\u001b[0m \u001b[39mreturn\u001b[39;00m columns_to_dtypes, signatures\n","\u001b[1;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'as_list'"]}],"source":["tf_train_set = tokenized_data_2[\"train\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n","    shuffle=True,\n","    batch_size=16,\n","    collate_fn=data_collator,\n",")\n","\n","tf_test_set = tokenized_data_2[\"test\"].to_tf_dataset(\n","    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n","    shuffle=False,\n","    batch_size=16,\n","    collate_fn=data_collator,\n",")\n","\n","from transformers import create_optimizers, AdamWeightDecay\n","\n","optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n","\n","from transformers import TFAutoModelForSeq2SeqLM\n","\n","#data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n","model.compile(optimizer=optimizer)\n","\n","model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)\n"]},{"cell_type":"markdown","metadata":{"id":"trAHeB19zFxc"},"source":["#-=-=-=-\n","##End Train with Transformers\n","\n","#-=-=-=-\n","\n","#Begin Trin with RNN\n","\n","#-=-=-=-"]},{"cell_type":"markdown","metadata":{"id":"9_97tFGGFIvv"},"source":["Load the texts for the source-language, here we use Indonesian."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"MDmiLM8VFIvv","scrolled":true},"outputs":[],"source":["data_src = dataset_rnn[s_lang]\n","\n","#europarl.load_data(english=False,\n"," #                             language_code=language_code)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DSgdvUZ9FIvw"},"source":["Load the texts for the destination-language, here we use English."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ICMR6RpUFIvw"},"outputs":[],"source":["data_dest = dataset_rnn[d_lang]\n","for i in range(len(data_dest)):\n","  data_dest[i] = mark_start + str(data_dest[i]) + mark_end\n","#data_dest = [mark_start + str(example) for example in _dataset_micro['test'][d_lang] + mark_end]\n","\n","#europarl.load_data(english=True,\n"," #                              language_code=language_code,\n","  #                             start=mark_start,\n","   #                            end=mark_end)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tetapi juga peraih medali Marit Bouwmeester, Edith Bosch dan Laura Smulders akan ada di sana.\n","-=-=-=-\n","ssss But also medalists Marit Bouwmeester, Edith Bosch and Laura Smulders will be there. eeee\n"]}],"source":["print(data_src[1])\n","print(\"-=-=-=-\")\n","print(data_dest[1])"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4996\n","107840\n","119047\n","123116\n","133128\n","182855\n","204968\n","364687\n","413849\n","437767\n","482977\n","504479\n","664885\n","752570\n","773447\n","844543\n","851807\n","898074\n","True\n"]}],"source":["null_count_src = 0\n","for i in range(len(data_src)):\n","    if (type(data_src[i]) != str):\n","        data_src[i] = \"kosong\"\n","        data_dest[i] = \"ssss empty eeee\"\n","        print(i)\n","        null_count_src += 1\n","\n","print(type(data_src[0]) == str)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["data_src = [example.replace(\",\", \" ,\").replace(\";\", \" ;\").replace(':', \" :\").replace('.', ' .').replace('?', \" ?\").replace(\"!\", \" !\").replace(\"'\", \" '\").replace(\"(\", \" (\").replace(\")\", \" )\") for example in data_src]\n","data_dest = [example.replace(\",\", \" ,\").replace(\";\", \" ;\").replace(':', \" :\").replace('.', ' .').replace('?', \" ?\").replace(\"!\", \" !\").replace(\"'\", \" '\").replace(\"(\", \" (\").replace(\")\", \" )\") for example in data_dest]"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Akhirnya dia mulai mendapatkan suara dan inspirasi yang dia nyatakan sebagai milik kita - saya menulis kepadanya banyak surat peringatan dan penjelasan yang serius tetapi dia menolak untuk mendengarkan , terlalu terikat pada suara palsu dan inspirasi dan , untuk menghindari teguran dan koreksi .  , berhenti menulis atau memberi tahu kami .\n","-=-=-=-\n","ssss At last he began to get voices and inspirations which he declared to be ours – I wrote to him many letters of serious warning and explanation but he refused to listen , was too much attached to his false voices and inspirations and , to avoid rebuke and correction , ceased to write or inform us . eeee\n"]}],"source":["print(data_src[1])\n","print(\"-=-=-=-\")\n","print(data_dest[1])"]},{"cell_type":"markdown","metadata":{"id":"Yfup10qcFIvw"},"source":["We will build a model to translate from the source language (English) to the destination language (Indonesian). If you want to make the inverse translation you can merely exchange the source and destination data."]},{"cell_type":"markdown","metadata":{"id":"BHqO-WAbFIvx"},"source":["### Example Data\n","\n","The data is just a list of texts that is ordered so the source and destination texts match. I can confirm that this example is an accurate translation."]},{"cell_type":"code","execution_count":81,"metadata":{"id":"EyDKoFR_FIvx"},"outputs":[],"source":["idx = 10"]},{"cell_type":"code","execution_count":82,"metadata":{"id":"k94DgCn1FIvx","outputId":"bc65de39-cad8-4208-ec08-9b9f3dff6eef"},"outputs":[{"data":{"text/plain":["'Buat cadangan dan beri label .'"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["data_src[idx]"]},{"cell_type":"code","execution_count":83,"metadata":{"id":"MUa95reOFIvy","outputId":"83634891-ca58-40ea-f5db-ad7d7f9d7573"},"outputs":[{"data":{"text/plain":["'ssss Create a backup and label it . eeee'"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["data_dest[idx]"]},{"cell_type":"markdown","metadata":{"id":"h3BcgjLLFIv0"},"source":["## Tokenizer\n","\n","Neural Networks cannot work directly on text-data. We use a two-step process to convert text into numbers that can be used in a neural network. The first step is to convert text-words into so-called integer-tokens. The second step is to convert integer-tokens into vectors of floating-point numbers using a so-called embedding-layer. See Tutorial #20 for a more detailed explanation.\n","\n","Set the maximum number of words in our vocabulary. This means that we will only use e.g. the 10000 most frequent words in the data-set. We use the same number for both the source and destination languages, but these could be different."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"yGaQvoJUFIv0"},"outputs":[],"source":["num_words = 10000\n","filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["filters='#$%&*+-/<=>@[\\\\]^_{|}~\\t\\n'"]},{"cell_type":"markdown","metadata":{"id":"TkMLge9vFIv0"},"source":["We need a few more functions than provided by Keras' Tokenizer-class so we wrap it."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"elapsed":360,"status":"error","timestamp":1646958493504,"user":{"displayName":"Jay Pankey","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12809569194307123660"},"user_tz":360},"id":"U1mJUiNBFIv1","outputId":"6d21e8fd-6d8c-401e-9e21-2fb9ec0b45d5"},"outputs":[],"source":["class TokenizerWrap(Tokenizer):\n","    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n","    \n","    def __init__(self, texts, padding,\n","                 reverse=False, num_words=None):\n","        \"\"\"\n","        :param texts: List of strings. This is the data-set.\n","        :param padding: Either 'post' or 'pre' padding.\n","        :param reverse: Boolean whether to reverse token-lists.\n","        :param num_words: Max number of words to use.\n","        \"\"\"\n","\n","        Tokenizer.__init__(self, num_words=num_words, filters=filters)\n","\n","        # Create the vocabulary from the texts.\n","        self.fit_on_texts(texts)\n","\n","        # Create inverse lookup from integer-tokens to words.\n","        self.index_to_word = dict(zip(self.word_index.values(),\n","                                      self.word_index.keys()))\n","\n","        # Convert all texts to lists of integer-tokens.\n","        # Note that the sequences may have different lengths.\n","        self.tokens = self.texts_to_sequences(texts)\n","\n","        if reverse:\n","            # Reverse the token-sequences.\n","            self.tokens = [list(reversed(x)) for x in self.tokens]\n","        \n","            # Sequences that are too long should now be truncated\n","            # at the beginning, which corresponds to the end of\n","            # the original sequences.\n","            truncating = 'pre'\n","        else:\n","            # Sequences that are too long should be truncated\n","            # at the end.\n","            truncating = 'post'\n","\n","        # The number of integer-tokens in each sequence.\n","        self.num_tokens = [len(x) for x in self.tokens]\n","\n","        # Max number of tokens to use in all sequences.\n","        # We will pad / truncate all sequences to this length.\n","        # This is a compromise so we save a lot of memory and\n","        # only have to truncate maybe 5% of all the sequences.\n","        self.max_tokens = np.mean(self.num_tokens) \\\n","                          + 2 * np.std(self.num_tokens)\n","        self.max_tokens = int(self.max_tokens)\n","\n","        # Pad / truncate all token-sequences to the given length.\n","        # This creates a 2-dim numpy matrix that is easier to use.\n","        self.tokens_padded = pad_sequences(self.tokens,\n","                                           maxlen=self.max_tokens,\n","                                           padding=padding,\n","                                           truncating=truncating)\n","\n","    def token_to_word(self, token):\n","        \"\"\"Lookup a single word from an integer-token.\"\"\"\n","\n","        word = \" \" if token == 0 else self.index_to_word[token]\n","        return word \n","\n","    def tokens_to_string(self, tokens):\n","        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n","\n","        # Create a list of the individual words.\n","        words = [self.index_to_word[token]\n","                 for token in tokens\n","                 if token != 0]\n","        \n","        # Concatenate the words to a single string\n","        # with space between all the words.\n","        text = \" \".join(words)\n","\n","        return text\n","    \n","    def text_to_tokens(self, text, reverse=False, padding=False):\n","        \"\"\"\n","        Convert a single text-string to tokens with optional\n","        reversal and padding.\n","        \"\"\"\n","\n","        # Convert to tokens. Note that we assume there is only\n","        # a single text-string so we wrap it in a list.\n","        tokens = self.texts_to_sequences([text])\n","        tokens = np.array(tokens)\n","\n","        if reverse:\n","            # Reverse the tokens.\n","            tokens = np.flip(tokens, axis=1)\n","\n","            # Sequences that are too long should now be truncated\n","            # at the beginning, which corresponds to the end of\n","            # the original sequences.\n","            truncating = 'pre'\n","        else:\n","            # Sequences that are too long should be truncated\n","            # at the end.\n","            truncating = 'post'\n","\n","        if padding:\n","            # Pad and truncate sequences to the given length.\n","            tokens = pad_sequences(tokens,\n","                                   maxlen=self.max_tokens,\n","                                   padding='pre',\n","                                   truncating=truncating)\n","\n","        return tokens"]},{"cell_type":"markdown","metadata":{"id":"Uze59pESFIv2"},"source":["Now create a tokenizer for the source-language. Note that we pad zeros at the beginning ('pre') of the sequences. We also reverse the sequences of tokens because the research literature suggests that this might improve performance, because the last words seen by the encoder match the first words produced by the decoder, so short-term dependencies are supposedly modelled more accurately."]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["null_count_dest = 0\n","for i in range(len(data_dest)):\n","    if (type(data_src[i]) != str):\n","        print(i)\n","        null_count_dest += 1\n","\n","print(type(data_src[0]) == str)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"Tz29bpw2FIv2"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 1min 24s\n","Wall time: 1min 28s\n"]}],"source":["%%time\n","tokenizer_src = TokenizerWrap(texts=data_src,\n","                              padding='pre',\n","                              reverse=True,\n","                              num_words=num_words)"]},{"cell_type":"markdown","metadata":{"id":"b_EYorK8FIv2"},"source":["Now create the tokenizer for the destination language. We need a tokenizer for both the source- and destination-languages because their vocabularies are different. Note that this tokenizer does not reverse the sequences and it pads zeros at the end ('post') of the arrays."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"wXAiY7zKFIv3"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 1min 41s\n","Wall time: 1min 48s\n"]}],"source":["%%time\n","tokenizer_dest = TokenizerWrap(texts=data_dest,\n","                               padding='post',\n","                               reverse=False,\n","                               num_words=num_words)"]},{"cell_type":"markdown","metadata":{"id":"HyzOwOCkFIv3"},"source":["Define convenience variables for the padded token sequences. These are just 2-dimensional numpy arrays of integer-tokens.\n","\n","Note that the sequence-lengths are different for the source and destination languages. This is because texts with the same meaning may have different numbers of words in the two languages. \n","\n","Furthermore, we have made a compromise when tokenizing the original texts in order to save a lot of memory. This means we only truncate about 5% of the texts."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"NPCLGa4HFIv4"},"outputs":[{"name":"stdout","output_type":"stream","text":["(999988, 41)\n","(999988, 45)\n"]}],"source":["tokens_src = tokenizer_src.tokens_padded\n","tokens_dest = tokenizer_dest.tokens_padded\n","print(tokens_src.shape)\n","print(tokens_dest.shape)"]},{"cell_type":"markdown","metadata":{"id":"AUrsARTsFIv4"},"source":["This is the integer-token used to mark the beginning of a text in the destination-language."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"D1_cKc5YFIv5"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["token_start = tokenizer_dest.word_index[mark_start.strip()]\n","token_start"]},{"cell_type":"markdown","metadata":{"id":"wpKKP1qXFIv5"},"source":["This is the integer-token used to mark the end of a text in the destination-language."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"MFHi3u0jFIv5"},"outputs":[{"data":{"text/plain":["2"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["token_end = tokenizer_dest.word_index[mark_end.strip()]\n","token_end"]},{"cell_type":"markdown","metadata":{"id":"zfSmwlE2FIv6"},"source":["### Example of Token Sequences"]},{"cell_type":"markdown","metadata":{"id":"hIYcCOGaFIv6"},"source":["This is the output of the tokenizer. Note how it is padded with zeros at the beginning (pre-padding)."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"tNZAd3RGFIv6"},"outputs":[],"source":["idx = 10"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ywacRdJwFIv6"},"outputs":[{"data":{"text/plain":["array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    2,  494,  730,\n","        163,    6,  679, 1910,   21,    1,  550, 1029,   48,  111,   18,\n","        541,  259,   91,    5,  359,  874,  510,   13])"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["tokens_src[idx]"]},{"cell_type":"markdown","metadata":{"id":"5rG8oWW0FIv7"},"source":["We can reconstruct the original text by converting each integer-token back to its corresponding word:"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"19wD2l_yFIv7"},"outputs":[{"data":{"text/plain":["'. label beri dan cadangan buat'"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_src.tokens_to_string(tokens_src[idx])"]},{"cell_type":"markdown","metadata":{"id":"wrSJAGpQFIv7"},"source":["This text is actually reversed, as can be seen when compared to the original text from the data-set:"]},{"cell_type":"code","execution_count":84,"metadata":{"id":"F6SR-dGjFIv8"},"outputs":[{"data":{"text/plain":["'Buat cadangan dan beri label .'"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["data_src[idx]"]},{"cell_type":"markdown","metadata":{"id":"gyc5ZmgrFIv8"},"source":["This is the sequence of integer-tokens for the corresponding text in the destination-language. Note how it is padded with zeros at the end (post-padding)."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"hi8B6-JpFIv8"},"outputs":[{"data":{"text/plain":["array([   1,    9,  696, 1197,    6, 1131,  113,  909,  581,   11,   46,\n","          6,    4,  512, 1364,    3,   33,    8, 1577,  364, 1514,    5,\n","          2,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","          0])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["tokens_dest[idx]"]},{"cell_type":"markdown","metadata":{"id":"a5xqWJ0kFIv8"},"source":["We can reconstruct the original text by converting each integer-token back to its corresponding word:"]},{"cell_type":"code","execution_count":86,"metadata":{"id":"6s33vkGVFIv9"},"outputs":[{"data":{"text/plain":["'ssss create a backup and label it . eeee'"]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_dest.tokens_to_string(tokens_dest[idx])"]},{"cell_type":"markdown","metadata":{"id":"i-MM44ZAFIv9"},"source":["Compare this to the original text from the data-set, which is almost identical except for punctuation marks and a few words such as \"dreaded millennium bug\". This is because we only use a vocabulary of the 10000 most frequent words in the data-set and those 3 words were apparently not used frequently enough to be included in the vocabulary, so they are merely skipped."]},{"cell_type":"code","execution_count":87,"metadata":{"id":"7cGukL0dFIv9","scrolled":true},"outputs":[{"data":{"text/plain":["'ssss Create a backup and label it . eeee'"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["data_dest[idx]"]},{"cell_type":"markdown","metadata":{"id":"qH6l8SzjFIv9"},"source":["### Training Data\n","\n","Now that the data-set has been converted to sequences of integer-tokens that are padded and truncated and saved in numpy arrays, we can easily prepare the data for use in training the neural network.\n","\n","The input to the encoder is merely the numpy array for the padded and truncated sequences of integer-tokens produced by the tokenizer:"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"zLpjWg53FIv9"},"outputs":[],"source":["encoder_input_data = tokens_src"]},{"cell_type":"markdown","metadata":{"id":"sHIxQ7JlFIv-"},"source":["The input and output data for the decoder is identical, except shifted one time-step. We can use the same numpy array to save memory by slicing it, which merely creates different 'views' of the same data in memory."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"6dPemARgFIv-"},"outputs":[{"data":{"text/plain":["(999988, 44)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["decoder_input_data = tokens_dest[:, :-1]\n","decoder_input_data.shape"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"pmubvuzYFIv-"},"outputs":[{"data":{"text/plain":["(999988, 44)"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["decoder_output_data = tokens_dest[:, 1:]\n","decoder_output_data.shape"]},{"cell_type":"markdown","metadata":{"id":"TeoMw4ZpFIv-"},"source":["For example, these token-sequences are identical except they are shifted one time-step."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"W-t3jZjWFIv-"},"outputs":[],"source":["idx = 2"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"zKFYxVjsFIv_"},"outputs":[{"data":{"text/plain":["array([   1,  298,    3,   60,    4, 2038, 3018, 1615,    3,   15,   32,\n","          4,  899,    6, 2552,   19,  181,    8,  591,    3,   23,   89,\n","         23,    4,  645,  591,    8,  181,    5,    2,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["decoder_input_data[idx]"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"zrFH_6PaFIv_"},"outputs":[{"data":{"text/plain":["array([ 298,    3,   60,    4, 2038, 3018, 1615,    3,   15,   32,    4,\n","        899,    6, 2552,   19,  181,    8,  591,    3,   23,   89,   23,\n","          4,  645,  591,    8,  181,    5,    2,    0,    0,    0,    0,\n","          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["decoder_output_data[idx]"]},{"cell_type":"markdown","metadata":{"id":"nxBbNXpKFIv_"},"source":["If we use the tokenizer to convert these sequences back into text, we see that they are identical except for the first word which is 'ssss' that marks the beginning of a text."]},{"cell_type":"code","execution_count":43,"metadata":{"id":"OHoUiQVnFIv_"},"outputs":[{"data":{"text/plain":["'ssss however , when the wild symbols appear , you have the option of winning from right to left , as well as the standard left to right . eeee'"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_dest.tokens_to_string(decoder_input_data[idx])"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"3Jf8ZT3pFIv_"},"outputs":[{"data":{"text/plain":["'however , when the wild symbols appear , you have the option of winning from right to left , as well as the standard left to right . eeee'"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer_dest.tokens_to_string(decoder_output_data[idx])"]},{"cell_type":"markdown","metadata":{"id":"wD1AErCFFIwA"},"source":["## Create the Neural Network\n","\n","### Create the Encoder\n","\n","First we create the encoder-part of the neural network which maps a sequence of integer-tokens to a \"thought vector\". We will use the so-called functional API of Keras for this, where we first create the objects for all the layers of the neural network and then we connect them later, this allows for more flexibility than the so-called sequential API in Keras, which is useful when experimenting with more complicated architectures and ways of connecting the encoder and decoder."]},{"cell_type":"markdown","metadata":{"id":"FJRfmMrNFIwA"},"source":["This is the input for the encoder which takes batches of integer-token sequences. The `None` indicates that the sequences can have arbitrary length."]},{"cell_type":"code","execution_count":45,"metadata":{"id":"W4LNtAwDFIwA"},"outputs":[],"source":["encoder_input = Input(shape=(None, ), name='encoder_input')"]},{"cell_type":"markdown","metadata":{"id":"Cl4kgngkFIwA"},"source":["This is the length of the vectors output by the embedding-layer, which maps integer-tokens to vectors of values roughly between -1 and 1, so that words that have similar semantic meanings are mapped to vectors that are similar. See Tutorial #20 for a more detailed explanation of this."]},{"cell_type":"code","execution_count":46,"metadata":{"id":"3WeYts7RFIwA"},"outputs":[],"source":["embedding_size = 128"]},{"cell_type":"markdown","metadata":{"id":"bvu90FMmFIwB"},"source":["This is the embedding-layer."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"OmJoZwtrFIwB"},"outputs":[],"source":["encoder_embedding = Embedding(input_dim=num_words,\n","                              output_dim=embedding_size,\n","                              name='encoder_embedding')"]},{"cell_type":"markdown","metadata":{"id":"36NZ1cS9FIwB"},"source":["This is the size of the internal states of the Gated Recurrent Units (GRU). The same size is used in both the encoder and decoder."]},{"cell_type":"code","execution_count":48,"metadata":{"id":"PDsnJabZFIwB"},"outputs":[],"source":["state_size = 512"]},{"cell_type":"markdown","metadata":{"id":"JUD0Zvg-FIwB"},"source":["This creates the 3 GRU layers that will map from a sequence of embedding-vectors to a single \"thought vector\" which summarizes the contents of the input-text. Note that the last GRU-layer does not return a sequence."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"6pxB0WQoFIwB"},"outputs":[],"source":["encoder_gru1 = GRU(state_size, name='encoder_gru1',\n","                   return_sequences=True)\n","encoder_gru2 = GRU(state_size, name='encoder_gru2',\n","                   return_sequences=True)\n","encoder_gru3 = GRU(state_size, name='encoder_gru3',\n","                   return_sequences=False)"]},{"cell_type":"markdown","metadata":{"id":"_mg8tYb2FIwC"},"source":["This helper-function connects all the layers of the encoder."]},{"cell_type":"code","execution_count":50,"metadata":{"id":"W9gSpnMWFIwC"},"outputs":[],"source":["def connect_encoder():\n","    # Start the neural network with its input-layer.\n","    net = encoder_input\n","    \n","    # Connect the embedding-layer.\n","    net = encoder_embedding(net)\n","\n","    # Connect all the GRU-layers.\n","    net = encoder_gru1(net)\n","    net = encoder_gru2(net)\n","    net = encoder_gru3(net)\n","\n","    # This is the output of the encoder.\n","    encoder_output = net\n","    \n","    return encoder_output"]},{"cell_type":"markdown","metadata":{"id":"aZnEf3QDFIwC"},"source":["Note how the encoder uses the normal output from its last GRU-layer as the \"thought vector\". Research papers often use the internal state of the encoder's last recurrent layer as the \"thought vector\". But this makes the implementation more complicated and is not necessary when using the GRU. But if you were using the LSTM instead then it is necessary to use the LSTM's internal states as the \"thought vector\" because it actually has two internal vectors, which we would need to initialize the two internal states of the decoder's LSTM units.\n","\n","We can now use this function to connect all the layers in the encoder so it can be connected to the decoder further below."]},{"cell_type":"code","execution_count":51,"metadata":{"id":"JWaSpJT4FIwC"},"outputs":[],"source":["encoder_output = connect_encoder()"]},{"cell_type":"markdown","metadata":{"id":"Rn6T-8oqFIwC"},"source":["### Create the Decoder\n","\n","Create the decoder-part which maps the \"thought vector\" to a sequence of integer-tokens.\n","\n","The decoder takes two inputs. First it needs the \"thought vector\" produced by the encoder which summarizes the contents of the input-text."]},{"cell_type":"code","execution_count":52,"metadata":{"id":"YsL_1962FIwD"},"outputs":[],"source":["decoder_initial_state = Input(shape=(state_size,),\n","                              name='decoder_initial_state')"]},{"cell_type":"markdown","metadata":{"id":"K3mV5MvWFIwD"},"source":["The decoder also needs a sequence of integer-tokens as inputs. During training we will supply this with a full sequence of integer-tokens e.g. corresponding to the text \"ssss once upon a time eeee\". \n","\n","During inference when we are translating new input-texts, we will start by feeding a sequence with just one integer-token for \"ssss\" which marks the beginning of a text, and combined with the \"thought vector\" from the encoder, the decoder will hopefully be able to produce the correct next word e.g. \"once\"."]},{"cell_type":"code","execution_count":53,"metadata":{"id":"sF6Ro5BrFIwD"},"outputs":[],"source":["decoder_input = Input(shape=(None, ), name='decoder_input')"]},{"cell_type":"markdown","metadata":{"id":"t4A5GAp6FIwD"},"source":["This is the embedding-layer which converts integer-tokens to vectors of real-valued numbers roughly between -1 and 1. Note that we have different embedding-layers for the encoder and decoder because we have two different vocabularies and two different tokenizers for the source and destination languages."]},{"cell_type":"code","execution_count":54,"metadata":{"id":"PijdVhVbFIwD"},"outputs":[],"source":["decoder_embedding = Embedding(input_dim=num_words,\n","                              output_dim=embedding_size,\n","                              name='decoder_embedding')"]},{"cell_type":"markdown","metadata":{"id":"zfcusEjjFIwD"},"source":["This creates the 3 GRU layers of the decoder. Note that they all return sequences because we ultimately want to output a sequence of integer-tokens that can be converted into a text-sequence."]},{"cell_type":"code","execution_count":55,"metadata":{"id":"2PdBHV2dFIwE"},"outputs":[],"source":["decoder_gru1 = GRU(state_size, name='decoder_gru1',\n","                   return_sequences=True)\n","decoder_gru2 = GRU(state_size, name='decoder_gru2',\n","                   return_sequences=True)\n","decoder_gru3 = GRU(state_size, name='decoder_gru3',\n","                   return_sequences=True)"]},{"cell_type":"markdown","metadata":{"id":"AkzQwZzNFIwE"},"source":["The GRU layers output a tensor with shape `[batch_size, sequence_length, state_size]`, where each \"word\" is encoded as a vector of length `state_size`. We need to convert this into sequences of integer-tokens that can be interpreted as words from our vocabulary.\n","\n","One way of doing this is to convert the GRU output to a one-hot encoded array. It works but it is extremely wasteful, because for a vocabulary of e.g. 10000 words we need a vector with 10000 elements, so we can select the index of the highest element to be the integer-token."]},{"cell_type":"code","execution_count":56,"metadata":{"id":"ZOVOSUmmFIwE"},"outputs":[],"source":["decoder_dense = Dense(num_words,\n","                      activation='softmax',\n","                      name='decoder_output')"]},{"cell_type":"markdown","metadata":{"id":"or6GU5ujFIwE"},"source":["The decoder is built using the functional API of Keras, which allows more flexibility in connecting the layers e.g. to route different inputs to the decoder. This is useful because we have to connect the decoder directly to the encoder, but we will also connect the decoder to another input so we can run it separately.\n","\n","This function connects all the layers of the decoder to some input of the initial-state values for the GRU layers."]},{"cell_type":"code","execution_count":57,"metadata":{"id":"8cJQ-VyKFIwJ"},"outputs":[],"source":["def connect_decoder(initial_state):\n","    # Start the decoder-network with its input-layer.\n","    net = decoder_input\n","\n","    # Connect the embedding-layer.\n","    net = decoder_embedding(net)\n","    \n","    # Connect all the GRU-layers.\n","    net = decoder_gru1(net, initial_state=initial_state)\n","    net = decoder_gru2(net, initial_state=initial_state)\n","    net = decoder_gru3(net, initial_state=initial_state)\n","\n","    # Connect the final dense layer that converts to\n","    # one-hot encoded arrays.\n","    decoder_output = decoder_dense(net)\n","    \n","    return decoder_output"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["dropout = tf.keras.layers.Dropout(0.5)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["def connect_decoder_dropout(initial_state):\n","    # Start the decoder-network with its input-layer.\n","    \n","    net = decoder_input\n","\n","    # Connect the embedding-layer.\n","    net = decoder_embedding(net)\n","    \n","    # Connect all the GRU-layers.\n","    net = decoder_gru1(net, initial_state=initial_state)\n","    net = decoder_gru2(net, initial_state=initial_state)\n","    net = decoder_gru3(net, initial_state=initial_state)\n","    net = dropout(net, training=True)\n","    # Connect the final dense layer that converts to\n","    # one-hot encoded arrays.\n","    decoder_output = decoder_dense(net)\n","    \n","    return decoder_output"]},{"cell_type":"markdown","metadata":{"id":"QlunCi-JFIwJ"},"source":["### Connect and Create the Models\n","\n","We can now connect the encoder and decoder in different ways.\n","\n","First we connect the encoder directly to the decoder so it is one whole model that can be trained end-to-end. This means the initial-state of the decoder's GRU units are set to the output of the encoder."]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["class MyModel(Model):\n","\n","  def __init__(self):\n","    super().__init__()\n","    self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n","    self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n","    self.dropout = tf.keras.layers.Dropout(0.5)\n","\n","  def call(self, inputs, training=False):\n","    x = self.dense1(inputs)\n","    if training:\n","      x = self.dropout(x, training=training)\n","    return self.dense2(x)\n","\n","_myModel = MyModel()"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"pK5CLW_xFIwJ"},"outputs":[],"source":["decoder_output = connect_decoder(initial_state=encoder_output)\n","dropout_decoder_output = connect_decoder_dropout(initial_state=encoder_output)\n","model_train = Model(inputs=[encoder_input, decoder_input],\n","                    outputs=[dropout_decoder_output])"]},{"cell_type":"markdown","metadata":{"id":"ISJQWQ_WFIwJ"},"source":["Then we create a model for just the encoder alone. This is useful for mapping a sequence of integer-tokens to a \"thought-vector\" summarizing its contents."]},{"cell_type":"code","execution_count":71,"metadata":{"id":"mJsf98v9FIwJ"},"outputs":[],"source":["model_encoder = Model(inputs=[encoder_input],\n","                      outputs=[encoder_output])"]},{"cell_type":"markdown","metadata":{"id":"_aQj6nBmFIwK"},"source":["Then we create a model for just the decoder alone. This allows us to directly input the initial state for the decoder's GRU units."]},{"cell_type":"code","execution_count":72,"metadata":{"id":"vdwFgPipFIwK"},"outputs":[],"source":["decoder_output = connect_decoder(initial_state=decoder_initial_state)\n","\n","model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n","                      outputs=[decoder_output])"]},{"cell_type":"markdown","metadata":{"id":"h5XxyERTFIwK"},"source":["Note that all these models use the same weights and variables of the encoder and decoder. We are merely changing how they are connected. So once the entire model has been trained, we can run the encoder and decoder models separately with the trained weights."]},{"cell_type":"markdown","metadata":{"id":"OLe-4rOpFIwK"},"source":["### Compile the Model\n","\n","The output of the decoder is a sequence of one-hot encoded arrays. In order to train the decoder we need to supply the one-hot encoded arrays that we desire to see on the decoder's output, and then use a loss-function like cross-entropy to train the decoder to produce this desired output.\n","\n","However, our data-set contains integer-tokens instead of one-hot encoded arrays. Each one-hot encoded array has 10000 elements so it would be extremely wasteful to convert the entire data-set to one-hot encoded arrays.\n","\n","A better way is to use a so-called sparse cross-entropy loss-function, which does the conversion internally from integers to one-hot encoded arrays.\n","\n","We have used the Adam optimizer in many of the previous tutorials, but it seems to diverge in some of these experiments with Recurrent Neural Networks. RMSprop seems to work much better for these."]},{"cell_type":"code","execution_count":73,"metadata":{"id":"k_hIzI0ZFIwK"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\panke\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(RMSprop, self).__init__(name, **kwargs)\n"]}],"source":["model_train.compile(optimizer=RMSprop(lr=1e-3),\n","                    loss='sparse_categorical_crossentropy')"]},{"cell_type":"markdown","metadata":{"id":"pd_qAoy6FIwK"},"source":["### Callback Functions\n","\n","During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n","\n","This is the callback for writing checkpoints during training."]},{"cell_type":"code","execution_count":74,"metadata":{"id":"vlBBiYCHFIwL"},"outputs":[],"source":["path_checkpoint = '21_checkpoint_dropout.keras'\n","callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n","                                      monitor='val_loss',\n","                                      verbose=1,\n","                                      save_weights_only=True,\n","                                      save_best_only=True)"]},{"cell_type":"markdown","metadata":{"id":"os1RxAmmFIwL"},"source":["This is the callback for stopping the optimization when performance worsens on the validation-set."]},{"cell_type":"code","execution_count":75,"metadata":{"id":"-VzYpm3PFIwL"},"outputs":[],"source":["callback_early_stopping = EarlyStopping(monitor='val_loss',\n","                                        patience=3, verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"U8HC78t2FIwL"},"source":["This is the callback for writing the TensorBoard log during training."]},{"cell_type":"code","execution_count":76,"metadata":{"id":"PGgRCptNFIwM"},"outputs":[],"source":["callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n","                                   histogram_freq=0,\n","                                   write_graph=False)"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"pG5ohCIGFIwM"},"outputs":[],"source":["callbacks = [callback_early_stopping,\n","             callback_checkpoint,\n","             callback_tensorboard]"]},{"cell_type":"markdown","metadata":{"id":"bvC8xa_XFIwM"},"source":["### Load Checkpoint\n","\n","You can reload the last saved checkpoint so you don't have to train the model every time you want to use it."]},{"cell_type":"code","execution_count":78,"metadata":{"id":"Jx_kcqeVFIwN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error trying to load checkpoint.\n","Unable to open file (file signature not found)\n"]}],"source":["try:\n","    model_train.load_weights(path_checkpoint)\n","except Exception as error:\n","    print(\"Error trying to load checkpoint.\")\n","    print(error)"]},{"cell_type":"markdown","metadata":{"id":"1Pst0G4FFIwN"},"source":["### Train the Model\n","\n","We wrap the data in named dicts so we are sure the data is assigned correctly to the inputs and outputs of the model."]},{"cell_type":"code","execution_count":79,"metadata":{"id":"dGUDpnCUFIwN"},"outputs":[],"source":["x_data = \\\n","{\n","    'encoder_input': encoder_input_data,\n","    'decoder_input': decoder_input_data\n","}"]},{"cell_type":"code","execution_count":80,"metadata":{"id":"k_JXeurLFIwN"},"outputs":[],"source":["y_data = \\\n","{\n","    'decoder_output': decoder_output_data\n","}"]},{"cell_type":"markdown","metadata":{"id":"_eLEmBfCFIwO"},"source":["We want a validation-set of 10000 sequences but Keras needs this number as a fraction."]},{"cell_type":"code","execution_count":81,"metadata":{"id":"H5tdQWQAFIwO","outputId":"e07c3b48-6aef-4265-8600-c2c72c8e6c73"},"outputs":[{"data":{"text/plain":["0.010000120001440018"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["validation_split = 10000 / len(encoder_input_data)\n","validation_split"]},{"cell_type":"markdown","metadata":{"id":"Zs8tsVgXFIwO"},"source":["Now we can train the model. One epoch of training took about 1 hour on a GTX 1070 GPU. You probably need to run 10 epochs or more during training. After 10 epochs the loss was about 1.10 on the training-set and about 1.15 on the validation-set.\n","\n","The batch-size was chosen to keep the GPU running at nearly 100% while being within the memory limits of 8GB for this GPU."]},{"cell_type":"code","execution_count":82,"metadata":{"id":"4NpubSXbFIwP","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","   1/2579 [..............................] - ETA: 29:57:14 - loss: 9.2098"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\panke\\Documents\\College\\Year 4 - 2021-2022\\Semester 2\\COMP 5650 - Deep Learning\\Comp-5650-Final-Project\\21_Machine_Translation.ipynb Cell 162'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000157?line=0'>1</a>\u001b[0m model_train\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mx_data,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000157?line=1'>2</a>\u001b[0m                 y\u001b[39m=\u001b[39;49my_data,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000157?line=2'>3</a>\u001b[0m                 batch_size\u001b[39m=\u001b[39;49m\u001b[39m384\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000157?line=3'>4</a>\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000157?line=4'>5</a>\u001b[0m                 validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/panke/Documents/College/Year%204%20-%202021-2022/Semester%202/COMP%205650%20-%20Deep%20Learning/Comp-5650-Final-Project/21_Machine_Translation.ipynb#ch0000157?line=5'>6</a>\u001b[0m                 callbacks\u001b[39m=\u001b[39;49mcallbacks)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1376'>1377</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1377'>1378</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1378'>1379</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1379'>1380</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1380'>1381</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1381'>1382</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1382'>1383</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1383'>1384</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1384'>1385</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/keras/engine/training.py?line=1385'>1386</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=911'>912</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=913'>914</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=914'>915</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=916'>917</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=917'>918</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=943'>944</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=944'>945</a>\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=945'>946</a>\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=946'>947</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=947'>948</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=948'>949</a>\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=949'>950</a>\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/def_function.py?line=950'>951</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=2952'>2953</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=2953'>2954</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=2954'>2955</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=2955'>2956</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=2956'>2957</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1848'>1849</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1849'>1850</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1850'>1851</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1851'>1852</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1852'>1853</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1853'>1854</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1854'>1855</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1855'>1856</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1856'>1857</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1857'>1858</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=1858'>1859</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=496'>497</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=497'>498</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=498'>499</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=499'>500</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=500'>501</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=501'>502</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=502'>503</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=503'>504</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=504'>505</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=505'>506</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=506'>507</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=507'>508</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=510'>511</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/function.py?line=511'>512</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/execute.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/execute.py?line=52'>53</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/execute.py?line=53'>54</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/execute.py?line=54'>55</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/execute.py?line=55'>56</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///c%3A/Users/panke/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0/LocalCache/local-packages/Python39/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model_train.fit(x=x_data,\n","                y=y_data,\n","                batch_size=384,\n","                epochs=10,\n","                validation_split=validation_split,\n","                callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"yiMVBBYk23la"},"source":["#-=-=-=-\n","#End Train on RNN\n","\n","\n","\n","#-=-=-=-\n","\n","\n","#Begin Translate (Both RNN and Transformer)\n","\n","#-=-=-=-"]},{"cell_type":"markdown","metadata":{"id":"T0XEPbsbFIwP"},"source":["## Translate Texts\n","\n","This function translates a text from the source-language to the destination-language and optionally prints a true translation."]},{"cell_type":"code","execution_count":253,"metadata":{"id":"KwBwuwQNFIwP"},"outputs":[],"source":["def translate(input_text, true_output_text=None):\n","    \"\"\"Translate a single text-string.\"\"\"\n","\n","    # Convert the input-text to integer-tokens.\n","    # Note the sequence of tokens has to be reversed.\n","    # Padding is probably not necessary.\n","    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n","                                                reverse=True,\n","                                                padding=True)\n","    \n","    # Get the output of the encoder's GRU which will be\n","    # used as the initial state in the decoder's GRU.\n","    # This could also have been the encoder's final state\n","    # but that is really only necessary if the encoder\n","    # and decoder use the LSTM instead of GRU because\n","    # the LSTM has two internal states.\n","    initial_state = model_encoder.predict(input_tokens)\n","\n","    # Max number of tokens / words in the output sequence.\n","    max_tokens = tokenizer_dest.max_tokens\n","\n","    # Pre-allocate the 2-dim array used as input to the decoder.\n","    # This holds just a single sequence of integer-tokens,\n","    # but the decoder-model expects a batch of sequences.\n","    shape = (1, max_tokens)\n","    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n","\n","    # The first input-token is the special start-token for 'ssss '.\n","    token_int = token_start\n","\n","    # Initialize an empty output-text.\n","    output_text = ''\n","\n","    # Initialize the number of tokens we have processed.\n","    count_tokens = 0\n","\n","    # While we haven't sampled the special end-token for ' eeee'\n","    # and we haven't processed the max number of tokens.\n","    while token_int != token_end and count_tokens < max_tokens:\n","        # Update the input-sequence to the decoder\n","        # with the last token that was sampled.\n","        # In the first iteration this will set the\n","        # first element to the start-token.\n","        decoder_input_data[0, count_tokens] = token_int\n","\n","        # Wrap the input-data in a dict for clarity and safety,\n","        # so we are sure we input the data in the right order.\n","        x_data = \\\n","        {\n","            'decoder_initial_state': initial_state,\n","            'decoder_input': decoder_input_data\n","        }\n","\n","        # Note that we input the entire sequence of tokens\n","        # to the decoder. This wastes a lot of computation\n","        # because we are only interested in the last input\n","        # and output. We could modify the code to return\n","        # the GRU-states when calling predict() and then\n","        # feeding these GRU-states as well the next time\n","        # we call predict(), but it would make the code\n","        # much more complicated.\n","\n","        # Input this data to the decoder and get the predicted output.\n","        decoder_output = model_decoder.predict(x_data)\n","\n","        # Get the last predicted token as a one-hot encoded array.\n","        token_onehot = decoder_output[0, count_tokens, :]\n","        \n","        # Convert to an integer-token.\n","        token_int = np.argmax(token_onehot)\n","\n","        # Lookup the word corresponding to this integer-token.\n","        sampled_word = tokenizer_dest.token_to_word(token_int)\n","\n","        # Append the word to the output-text.\n","        output_text += \" \" + sampled_word\n","\n","        # Increment the token-counter.\n","        count_tokens += 1\n","\n","    # Sequence of tokens output by the decoder.\n","    output_tokens = decoder_input_data[0]\n","    \n","    # Print the input-text.\n","    print(\"Input text:\")\n","    print(input_text)\n","    print()\n","\n","    # Print the translated output-text.\n","    print(\"Translated text:\")\n","    print(output_text)\n","    print()\n","\n","    # Optionally print the true translated text.\n","    if true_output_text is not None:\n","        print(\"True output text:\")\n","        print(true_output_text)\n","        print()"]},{"cell_type":"markdown","metadata":{"id":"Wq0tF2O0FIwQ"},"source":["### Examples\n","\n","Translate a text from the training-data. This translation is quite good. Note how it is not identical to the translation from the training-data, but the actual meaning is similar."]},{"cell_type":"code","execution_count":323,"metadata":{"id":"wajb7B9OFIwR","outputId":"2827faec-e89f-4beb-cdba-ce4f9b34eaaa"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\panke\\AppData\\Local\\Temp\\ipykernel_14368\\169333720.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n"]},{"name":"stdout","output_type":"stream","text":["Input text:\n","Stena Line memiliki opsi paling banyak untuk keberangkatan dari Harwich ke Hook of Holland, dengan rata-rata 2 perjalanan per hari dan 45 perjalanan bulanan.\n","\n","Translated text:\n"," the route has a large selection of departure for the group from with a low week of north camera 2 per hour and a monthly resort eeee\n","\n","True output text:\n","ssss Stena Line has the most options for departures from Harwich to Hook of Holland, with an average of 2 trips per day and 45 monthly trips. eeee\n","\n"]}],"source":["idx = 25\n","translate(input_text=data_src[idx],\n","          true_output_text=data_dest[idx])"]},{"cell_type":"markdown","metadata":{"id":"s1bFa5z3FIwR"},"source":["Here is another example which is also a reasonable translation, although it has incorrectly translated the natural disasters. Note \"countries of the European Union\" has instead been translated as \"member states\" which are synonyms in this context."]},{"cell_type":"code","execution_count":324,"metadata":{"id":"QPTh96mYFIwR","outputId":"6084a493-8bfc-49b3-8141-461cad3f65bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\panke\\AppData\\Local\\Temp\\ipykernel_14368\\169333720.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n"]},{"name":"stdout","output_type":"stream","text":["Input text:\n","Sistem penyetelan internal dengan kunci penyetelan hex\n","\n","Translated text:\n"," internal system with key adjustment eeee\n","\n","True output text:\n","ssss Internal tuning system with hex tuning key eeee\n","\n"]}],"source":["idx = 4\n","translate(input_text=data_src[idx],\n","          true_output_text=data_dest[idx])"]},{"cell_type":"markdown","metadata":{"id":"XvSSatvAFIwS"},"source":["In this example we join two texts from the training-set. The model first sends this combined text through the encoder, which produces a \"thought-vector\" that seems to summarize both texts reasonably well so the decoder can produce a reasonable translation."]},{"cell_type":"code","execution_count":325,"metadata":{"id":"zBWm-mKKFIwS","outputId":"1329120e-39ab-43e2-b278-f6fe737aca4c","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\panke\\AppData\\Local\\Temp\\ipykernel_14368\\169333720.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n"]},{"name":"stdout","output_type":"stream","text":["Input text:\n","COOMPY \"Cookies\", oleh akunnya sendiri atau pihak ketiga yang dikontrak untuk menyediakan layanan pengukuran dapat menggunakan cookie saat Pengguna menavigasi melalui konten Situs Web.Sistem penyetelan internal dengan kunci penyetelan hex\n","\n","Translated text:\n"," the third party by the cookies or the following provide a use of cookies while the user is able to complete the system through the contents of the format system the browser is covered with a digital circuit eeee\n","\n","True output text:\n","ssss “Cookies” COOMPY, by own account or third party contracted to provide measurement services may make use of cookies when the User navigates through the contents of the Website. eeeessss Internal tuning system with hex tuning key eeee\n","\n"]}],"source":["idx = 3\n","translate(input_text=data_src[idx] + data_src[idx+1],\n","          true_output_text=data_dest[idx] + data_dest[idx+1])"]},{"cell_type":"markdown","metadata":{"id":"4SkmD1pWFIwS"},"source":["If we reverse the order of these two texts then the meaning is not quite so clear for the latter text."]},{"cell_type":"code","execution_count":276,"metadata":{"id":"18WnLyT0FIwT","outputId":"dd7a4f08-7083-443b-f5d6-3b626e7b307a"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\panke\\AppData\\Local\\Temp\\ipykernel_14368\\169333720.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n"]},{"name":"stdout","output_type":"stream","text":["Input text:\n","Sistem penyetelan internal dengan kunci penyetelan hexCOOMPY \"Cookies\", oleh akunnya sendiri atau pihak ketiga yang dikontrak untuk menyediakan layanan pengukuran dapat menggunakan cookie saat Pengguna menavigasi melalui konten Situs Web.\n","\n","Translated text:\n"," the use of the connection with the file format or the party for the reason that the use of the services can be changed via the user settings when the user is linked to the website eeee\n","\n","True output text:\n","ssss Internal tuning system with hex tuning key eeeessss “Cookies” COOMPY, by own account or third party contracted to provide measurement services may make use of cookies when the User navigates through the contents of the Website. eeee\n","\n"]}],"source":["idx = 3\n","translate(input_text=data_src[idx+1] + data_src[idx],\n","          true_output_text=data_dest[idx+1] + data_dest[idx])"]},{"cell_type":"markdown","metadata":{"id":"jxOlA2mvFIwT"},"source":["This is an example I made up. It is a quite broken translation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ck7idz3bFIwT","outputId":"2c4935a2-c40a-4a56-b751-fa4386c0f2d4"},"outputs":[],"source":["translate(input_text=\"der var engang et land der hed Danmark\",\n","          true_output_text='Once there was a country named Denmark')"]},{"cell_type":"markdown","metadata":{"id":"hh6FzzyDFIwU"},"source":["This is another example I made up. This is a better translation even though it is perhaps a more complicated text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Y5GC_GGFIwU","outputId":"f1f8fe1c-cc5c-4081-f346-61f65244171a"},"outputs":[],"source":["translate(input_text=\"Idag kan man læse i avisen at Danmark er blevet fornuftigt\",\n","          true_output_text=\"Today you can read in the newspaper that Denmark has become sensible.\")"]},{"cell_type":"markdown","metadata":{"id":"FReoYVu5FIwU"},"source":["This is a text from a Danish song. It doesn't even make much sense in Danish. However the translation is probably so broken because several of the words are not in the vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WqGFCBmoFIwV","outputId":"3c7633fb-c367-41dd-8128-cf40b30520cf"},"outputs":[],"source":["translate(input_text=\"Hvem spæner ud af en butik og tygger de stærkeste bolcher?\",\n","          true_output_text=\"Who runs out of a shop and chews the strongest bon-bons?\")"]},{"cell_type":"markdown","metadata":{"id":"yAmxOrVNFIwV"},"source":["## Conclusion\n","\n","This tutorial showed the basic idea of using two Recurrent Neural Networks in a so-called encoder/decoder model to do Machine Translation of human languages. It was demonstrated on the very large Europarl data-set from the European Union.\n","\n","The model could produce reasonable translations for some texts but not for others. It is possible that a better architecture for the neural network and more training epochs could improve performance. There are also more advanced models that are known to improve quality of the translations.\n","\n","However, it is important to note that these models do not really understand human language. The models have no knowledge of the actual meaning of the words. The models are merely very advanced function approximators that can map between sequences of integer-tokens."]},{"cell_type":"markdown","metadata":{"id":"sFYaT8JyFIwV"},"source":["## Exercises\n","\n","These are a few suggestions for exercises that may help improve your skills with TensorFlow. It is important to get hands-on experience with TensorFlow in order to learn how to use it properly.\n","\n","You may want to backup this Notebook before making any changes.\n","\n","* Train for more than 10 epochs. Does it improve the translations?\n","* Increase the size of the vocabulary. Does it improve the translations? Would it make sense to have different sizes for the vocabularies of the source and destination languages?\n","* Find another data-set and use it together with Europarl.\n","* Change the architectures of the neural network, for example change the state-size for the GRU layers, the number of GRU layers, the embedding-size, etc. Does it improve the translations?\n","* Use hyper-parameter optimization from Tutorial #19 to automatically find the best hyper-parameters.\n","* When translating texts, instead of using `np.argmax()` to sample the next integer-token, could you sample the decoder's output as if it was a probability distribution instead? Note that the decoder's output is not softmax-limited so you have to do that first to turn it into a probability-distribution.\n","* Can you generate multiple sequences by doing this sampling? Can you find a way to select the best of these different sequences?\n","* Disable the reversal of words for the source-language. Does it improve the translations?\n","* What is a Bi-Directional GRU and can you use it here?\n","* We use the **output** of the encoder's GRU as the initial state of the decoder's GRU. The research literature often uses an LSTM instead of the GRU, so they used the encoder's **state** instead of its output as the initial state of the decoder. Can you rewrite this code to use the encoder's state as the decoder's initial state? Is there a reason to do this, or is the encoder's output sufficient to use as the decoder's initial state?\n","* Is it possible to connect multiple encoders and decoders in a single neural network, so that you can train it on different languages and allow for direct translation e.g. from Danish to Polish, German and French?\n","* Explain to a friend how the program works."]},{"cell_type":"markdown","metadata":{"id":"I5gTD_H4FIwW"},"source":["## License (MIT)\n","\n","Copyright (c) 2018 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n","\n","Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n","\n","The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n","\n","THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."]}],"metadata":{"anaconda-cloud":{},"colab":{"collapsed_sections":["qH6l8SzjFIv9","wD1AErCFFIwA","Rn6T-8oqFIwC","QlunCi-JFIwJ","OLe-4rOpFIwK","pd_qAoy6FIwK","bvC8xa_XFIwM","1Pst0G4FFIwN","T0XEPbsbFIwP","Wq0tF2O0FIwQ","yAmxOrVNFIwV","sFYaT8JyFIwV","I5gTD_H4FIwW"],"name":"21_Machine_Translation.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0aa9a7fd8caf4ec9bfca3e068a9bd00d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4eba7b6a6a34a2aa93e7201e47564a5","placeholder":"​","style":"IPY_MODEL_db4230021f97435396ac427400dbabfc","value":"100%"}},"0e70b29c14f04d90a01581b0d8126364":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b43c4170f2e40a99e6a8cba1ce0a2d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27e71996fd8b45ed8e17ed013728728d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0aa9a7fd8caf4ec9bfca3e068a9bd00d","IPY_MODEL_e2467eb99aae4404a4bf9377c4906451","IPY_MODEL_9f508c40f4b74a50bf4205fda6df06c3"],"layout":"IPY_MODEL_92684d53ff5d48f886db72d6e9f66081"}},"3ad9a88eff574ef4b09eb74472810410":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40844d7d58654cbab7373e2ae7589229":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4288cd7a677f4a79bf6af19c52aff64b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e70b29c14f04d90a01581b0d8126364","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ad9a88eff574ef4b09eb74472810410","value":0}},"471ac32f51254cb2afed0a535fb29f16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52dc7ef1f16e471998ea714c5c11297c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ba67a4e8ae94e3b887d6a82c976f8e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fe6122d9a6e4b2f9988525007b95d70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a57acaa9a4124464acd4d5581b6a976f","IPY_MODEL_4288cd7a677f4a79bf6af19c52aff64b","IPY_MODEL_f41ff3961e7a491a832fbcb0711f8f41"],"layout":"IPY_MODEL_d4890aee06034969a85fa1362e331fee"}},"92684d53ff5d48f886db72d6e9f66081":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f508c40f4b74a50bf4205fda6df06c3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cee6be8bc3ff4d6eb70bf0971d053661","placeholder":"​","style":"IPY_MODEL_5ba67a4e8ae94e3b887d6a82c976f8e2","value":" 1/1 [00:00&lt;00:00, 24.31it/s]"}},"a4eba7b6a6a34a2aa93e7201e47564a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a57acaa9a4124464acd4d5581b6a976f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40844d7d58654cbab7373e2ae7589229","placeholder":"​","style":"IPY_MODEL_52dc7ef1f16e471998ea714c5c11297c","value":"  0%"}},"aac0261e8b4b44b8b177f2a9f8a295ef":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cab347bf122d468d89c9b387b280c874":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cee6be8bc3ff4d6eb70bf0971d053661":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4890aee06034969a85fa1362e331fee":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db4230021f97435396ac427400dbabfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2467eb99aae4404a4bf9377c4906451":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b43c4170f2e40a99e6a8cba1ce0a2d5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cab347bf122d468d89c9b387b280c874","value":1}},"f41ff3961e7a491a832fbcb0711f8f41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aac0261e8b4b44b8b177f2a9f8a295ef","placeholder":"​","style":"IPY_MODEL_471ac32f51254cb2afed0a535fb29f16","value":" 0/1 [00:00&lt;?, ?it/s]"}}}}},"nbformat":4,"nbformat_minor":0}
